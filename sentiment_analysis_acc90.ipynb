{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Sentiment Analysis Acc-90\n",
    "\n",
    "This notebook provides a simple straight-forward way to achieve 90% accuracy on IMDB dataset. Note that this is not the only way to achieve such accuracy. Also, this is far from the best accuracy compared with some more advanced STOA methods. I encourage you to try other methods on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_proj_utils as utils\n",
    "\n",
    "# Function for loading imdb dataset\n",
    "def load_imdb():\n",
    "    train, test = utils.get_imdb_dataset()\n",
    "    TEXT_COL, LABEL_COL = 'text', 'sentiment'\n",
    "    return (\n",
    "        train[TEXT_COL], train[LABEL_COL],\n",
    "        test[TEXT_COL], test[LABEL_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_label, test_text, test_label = load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Shape, should not throw exceptions\n",
    "for data in train_text, train_label, test_text, test_label:\n",
    "    assert data.shape == (25000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "\n",
    "### Build Vectorizer\n",
    "\n",
    "Instead of building vocabulary on our own, we will use the build-in method for generating vocabulary and doing vectorization provided by `TfidfVectorizer` from Sklearn. Before you continue reading, I would suggest you take a look at the documentation for `TfidfVectorizer` available [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "**Note**: \n",
    "\n",
    "It's always a good practice to read the documentation carefully instead of simply grab something and plug it in your project. It's important to understand what options are available and what is the default behavior. The rule of thumb is that, if you don't know what is the exact behavior of something, don't use it. Otherwise, get ready for endless time of debugging ðŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, # ignore word that only appears in 1 document\n",
    "    ngram_range=(1, 2), # consider both uni-gram and bi-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn (fit) and transform text into vector\n",
    "train_x = tfidf_vectorizer.fit_transform(train_text)\n",
    "\n",
    "# Convert label to 0 and 1 (optional)\n",
    "train_y = train_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "print('Training x shape:', train_x.shape)\n",
    "print('Training y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worry about the size of the vocabulary? Stay calm, we will take care of this horrible dimention later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect 12500 for 1 and 0, instead of pos and neg\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformer to validation set as well\n",
    "# Simply call `transform` this time, don't do `fit` again\n",
    "test_x = tfidf_vectorizer.transform(test_text)\n",
    "test_y = test_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert test_x.shape == train_x.shape\n",
    "assert test_y.shape == train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "There are a couple of ways to reduce the dimensionality. During the class, we introduced two ways. \n",
    "\n",
    "- One is to set a hard cut off on the number words in our vocabulary. In class, we only kept the top 3000 frequent words in the vocabulary. \n",
    "- The other way is to use a pretrained word embeddings. This is based on transfer learning, where we reduce the dimensions by learning the relation between words from another training task (e.g.: Word2Vec). \n",
    "\n",
    "So what's the pro and cons for both methods?\n",
    "\n",
    "- One defact about the first approach is that it didn't make use of the labels, i.e.: the dimensionality reduction was applied solely on information about the text. \"Top words are more useful\" is nothing more than one of our hypothesis. We have no idea on what words are more useful when doing classification.\n",
    "- The second approach is better. But one potential problem is that we kept it fixed, meaning that it may not fit in to our problem very well. One solution is to re-train the word embedding together with the weights of the network (simply set `trainable=True`). The training may take a long time thus a GPU is highly recommended.\n",
    "\n",
    "In this notebook, we decided to use another more general way to reduce dimensionality. We will be using `SelectKBest` from `sklearn` and using `f_classif` to help up pick up k best features (word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 20000 # Dimensions to keep, a hyper parameter\n",
    "\n",
    "# Create a feature selector\n",
    "# By default, f_classif algorithm is used\n",
    "# Other available options include mutual_info_classif, chi2, f_regression etc. \n",
    "\n",
    "selector = SelectKBest(k=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature selector also requires information from labels\n",
    "# Fit on training data\n",
    "selector.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to both training data and testing data\n",
    "train_x = selector.transform(train_x)\n",
    "test_x = selector.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert train_x.shape == (25000, 20000)\n",
    "assert test_x.shape == (25000, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a MLP Model\n",
    "\n",
    "Muti-Layer Perceptron model, aka Feed Forward Network, is the most basic neural network structure, but is used in quite a lot of place as it is very robust. It is true that deep networks are usually more powerful, but they are usually more data hungry. Since we don't have much data, a MLP model may works better in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is quite similar to `build_nn_model` in previous notebook,\n",
    "# except that we add an extra dropout layer after each dense layer\n",
    "\n",
    "def build_mlp_model(input_dim, layers, output_dim, dropout_rate=0.2):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "        H = Dropout(rate=dropout_rate)(H)\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'learning_rate': 1e-3,  # default for Adam\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 64,\n",
    "    'layers': [64, 32],\n",
    "    'dim': DIM,\n",
    "    'dropout_rate': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = build_mlp_model(\n",
    "    input_dim=hyper_params['dim'],\n",
    "    layers=hyper_params['layers'],\n",
    "    output_dim=1,\n",
    "    dropout_rate=hyper_params['dropout_rate'],\n",
    ")\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(\n",
    "    optimizer=Adam(lr=hyper_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks (aka hooks) are functions called every N epochs that help you monitor and log the training process. By default, they will be called every 1 epoch. We will be using two common callbacks here: `EarlyStopping` and `ModelCheckpoint`. The first is used to prevent overfitting and the second is used to keep track of the best models we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=2,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model, Hope for the Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = mlp_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs=10,\n",
    "    validation_data=[test_x, test_y],\n",
    "    batch_size=hyper_params['batch_size'],\n",
    "    callbacks=[early_stoppping_hook, model_cpk_hook],\n",
    ")\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Load the best model and do evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "mlp_model.load_weights(CPK_PATH)\n",
    "\n",
    "# Accuracy on validation \n",
    "mlp_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see results similar to the following screenshot. It's normal that the exact number may different as there are some randomization introduced during batching.\n",
    "\n",
    "<img src='resources/example_validation.png'>\n",
    "\n",
    "This isn't the end of tunning! Actually, we haven't done any hyper parameter tunning yet! Feel free to change some parameter settings and see if you can get a even better model :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
